{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12206965,"sourceType":"datasetVersion","datasetId":7689713},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Thanks to : https://www.kaggle.com/code/senkin13/notebookef94e473e0 and http://kaggle.com/code/kononenko/pip-install-no-internet for the package usage guide.\n### Thanks to : https://www.kaggle.com/code/richolson/smiles-rdkit-lgbm-ftw, https://www.kaggle.com/datasets/minatoyukinaxlisa/smiles-tg for the additional datasets.","metadata":{}},{"cell_type":"markdown","source":"### Baseline XGBoost with RDKit Feature Extraction : https://www.kaggle.com/code/digantabhattacharya/neurips25-opp-with-rdkit-xgb-starter-i/\n### Baseline LGBM with RDKit Feature Extraction : https://www.kaggle.com/code/digantabhattacharya/neurips25-rdkit-lgbm-additionaldata-starter-ii/","metadata":{}},{"cell_type":"markdown","source":"### Download the wheel as a dataset or use a notebook with internet as utility (But the later needs extensive version control.)","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:04:32.375768Z","iopub.execute_input":"2025-06-21T12:04:32.376133Z","iopub.status.idle":"2025-06-21T12:04:38.504184Z","shell.execute_reply.started":"2025-06-21T12:04:32.376110Z","shell.execute_reply":"2025-06-21T12:04:38.503269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Install usual packages: We will be just using rdkit and XGBoost. This is just a baseline to get started.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom typing import List\nimport warnings\nimport logging\nimport os\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Descriptors\nfrom rdkit.Chem import rdMolDescriptors\nfrom rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nfrom tqdm import tqdm\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:04:38.506046Z","iopub.execute_input":"2025-06-21T12:04:38.506286Z","iopub.status.idle":"2025-06-21T12:04:40.186345Z","shell.execute_reply.started":"2025-06-21T12:04:38.506263Z","shell.execute_reply":"2025-06-21T12:04:40.185871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Suppress warnings and set the logger ","metadata":{}},{"cell_type":"code","source":"# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set up logging with more detailed format\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler('polymer_prediction.log')\n    ]\n)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:04:42.581961Z","iopub.execute_input":"2025-06-21T12:04:42.582599Z","iopub.status.idle":"2025-06-21T12:04:42.587031Z","shell.execute_reply.started":"2025-06-21T12:04:42.582574Z","shell.execute_reply":"2025-06-21T12:04:42.586333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Check if GPU is available and return appropriate XGBoost parameters","metadata":{}},{"cell_type":"code","source":"def check_gpu_availability():\n    \"\"\"Check if GPU is available and return appropriate XGBoost parameters.\"\"\"\n    try:\n        # Try to create a simple XGBoost model to check GPU availability\n        xgb.XGBRegressor(tree_method='gpu_hist').get_params()\n        logger.info(\"GPU detected! Using GPU acceleration for XGBoost\")\n        return {\n            'tree_method': 'gpu_hist',\n            'gpu_id': 0,\n            'predictor': 'gpu_predictor'\n        }\n    except Exception as e:\n        logger.warning(f\"GPU not available: {str(e)}. Falling back to CPU\")\n        return {\n            'tree_method': 'hist',\n            'predictor': 'cpu_predictor'\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:04:44.221579Z","iopub.execute_input":"2025-06-21T12:04:44.221854Z","iopub.status.idle":"2025-06-21T12:04:44.226666Z","shell.execute_reply.started":"2025-06-21T12:04:44.221831Z","shell.execute_reply":"2025-06-21T12:04:44.225920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data loader with configuration if needed ","metadata":{}},{"cell_type":"code","source":"def load_data():\n    \"\"\"Load competition data and additional datasets.\n    \n    Returns:\n        Tuple: (train_smiles, train_targets, test_df)\n    \"\"\"\n    logger.info(\"Loading competition data...\")\n    \n    # Load training and test data\n    comp_train_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\n    test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n    logger.info(f\"Loaded {len(comp_train_df)} competition training samples and {len(test)} test samples\")\n    \n    # Load additional datasets\n    logger.info(\"Loading additional datasets...\")\n    extra_tg_file_path = \"/kaggle/input/smiles-tg/Tg_SMILES_class_pid_polyinfo_median.csv\"\n    extra_tc_file_path = \"/kaggle/input/tc-smiles/Tc_SMILES.csv\"\n    \n    extra_tg_df = pd.read_csv(extra_tg_file_path)\n    extra_tc_df = pd.read_csv(extra_tc_file_path)\n    logger.info(f\"Loaded {len(extra_tg_df)} additional Tg samples and {len(extra_tc_df)} additional Tc samples\")\n    \n    # Prepare extra_tg_df dataframe \n    extra_tg_clean = extra_tg_df[['SMILES', 'PID', 'Tg']].rename(columns={'PID': 'id'})\n    extra_tg_clean[['FFV', 'Tc', 'Density', 'Rg']] = float('nan')\n\n    # Prepare extra_tc_df  dataframe \n    extra_tc_clean = extra_tc_df[['SMILES', 'TC_mean']].rename(columns={'TC_mean': 'Tc'})\n    extra_tc_clean['id'] = range(len(comp_train_df) + len(extra_tg_df), len(comp_train_df) + len(extra_tg_df) + len(extra_tc_df))\n    extra_tc_clean[['Tg', 'FFV', 'Density', 'Rg']] = float('nan')\n\n    # Reorder columns to match comp_train_df dataframe\n    extra_tg_clean = extra_tg_clean[['id', 'SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']]\n    extra_tc_clean = extra_tc_clean[['id', 'SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']]\n\n    # Combine all datasets into train_df\n    train_df = pd.concat([comp_train_df, extra_tg_clean, extra_tc_clean], ignore_index=True)\n    logger.info(f\"Combined dataset has {len(train_df)} total training samples\")\n    \n    # Extract SMILES and target properties\n    train_smiles = train_df['SMILES'].values\n    train_targets = train_df[['Tg', 'FFV', 'Tc', 'Density', 'Rg']]\n    \n    # Log data statistics\n    for col in train_targets.columns:\n        valid_count = train_targets[col].notna().sum()\n        logger.info(f\"Property {col}: {valid_count} valid samples\")\n    \n    return train_smiles, train_targets, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:04:47.366391Z","iopub.execute_input":"2025-06-21T12:04:47.366687Z","iopub.status.idle":"2025-06-21T12:04:47.374172Z","shell.execute_reply.started":"2025-06-21T12:04:47.366665Z","shell.execute_reply":"2025-06-21T12:04:47.373460Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Basic Polymer Predictor Class Template with XGB & Optuna for hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"class PolymerPredictor:\n    def __init__(self, n_estimators: int = 1000, learning_rate: float = 0.01):\n        \"\"\"Initialize the polymer property predictor.\n        \n        Args:\n            n_estimators: Number of trees in XGBoost model\n            learning_rate: Learning rate for XGBoost\n        \"\"\"\n        logger.info(\"Initializing PolymerPredictor model...\")\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.models = {}\n        self.scalers = {}\n        \n        # Check GPU availability and set XGBoost parameters\n        self.xgb_params = check_gpu_availability()\n        \n        # Define reasonable ranges for each property\n        # These ranges help in normalizing the weighted MAE metric\n        self.property_ranges = {\n            'Tg': 500,      # Glass transition temperature range in K\n            'FFV': 0.5,     # Fractional free volume (0-1 range)\n            'Tc': 1000,     # Critical temperature range in K\n            'Density': 2.0, # Density range in g/cm³\n            'Rg': 100       # Radius of gyration range in Å\n        }\n        logger.info(f\"Model initialized with {n_estimators} trees and learning rate {learning_rate}\")\n        logger.info(f\"Using XGBoost parameters: {self.xgb_params}\")\n        \n    def _extract_features(self, smiles_list: List[str]) -> np.ndarray:\n        \"\"\"Extract chemical features from SMILES string using RDKit.\n        \n        Args:\n            smiles_list: List of SMILES strings\n            \n        Returns:\n            np.ndarray: Feature vector containing molecular descriptors\n        \"\"\"\n        logger.info(\"Starting feature extraction from SMILES strings...\")\n        features = []\n        # Use the new MorganGenerator API to avoid deprecation warnings\n        morgan_gen = GetMorganGenerator(radius=2, fpSize=1024)\n        \n        for smiles in tqdm(smiles_list, desc=\"Extracting features\"):\n            try:\n                # Convert SMILES to RDKit molecule\n                mol = Chem.MolFromSmiles(smiles)\n                if mol is None:\n                    logger.warning(f\"Invalid SMILES string: {smiles}\")\n                    features.append(np.zeros(200))  # Default feature vector\n                    continue\n                \n                # Calculate molecular descriptors\n                feature_vector = []\n                \n                # Basic descriptors\n                feature_vector.extend([\n                    Descriptors.MolWt(mol),\n                    Descriptors.NumRotatableBonds(mol),\n                    Descriptors.NumHDonors(mol),\n                    Descriptors.NumHAcceptors(mol),\n                    Descriptors.TPSA(mol),\n                    Descriptors.MolLogP(mol),\n                    Descriptors.NumAromaticRings(mol),\n                    Descriptors.NumAliphaticRings(mol),\n                    Descriptors.NumSaturatedRings(mol),\n                    Descriptors.NumHeteroatoms(mol)\n                ])\n                \n                # Morgan fingerprints (ECFP4) using the new generator API\n                fp = morgan_gen.GetFingerprint(mol).ToBitString()\n                fp_bits = [int(x) for x in fp]\n                feature_vector.extend(fp_bits)\n                \n                # Additional descriptors\n                feature_vector.extend([\n                    rdMolDescriptors.CalcNumRings(mol),\n                    rdMolDescriptors.CalcNumAromaticRings(mol),\n                    rdMolDescriptors.CalcNumAliphaticRings(mol),\n                    rdMolDescriptors.CalcNumSaturatedRings(mol),\n                    rdMolDescriptors.CalcNumHeterocycles(mol),\n                    rdMolDescriptors.CalcNumSpiroAtoms(mol),\n                    rdMolDescriptors.CalcNumBridgeheadAtoms(mol),\n                    rdMolDescriptors.CalcNumAtomStereoCenters(mol),\n                    rdMolDescriptors.CalcNumUnspecifiedAtomStereoCenters(mol)\n                ])\n                \n                # Pad or truncate to ensure consistent feature vector size\n                feature_vector = feature_vector[:200] + [0] * (200 - len(feature_vector))\n                features.append(feature_vector)\n                \n            except Exception as e:\n                logger.warning(f\"Error processing SMILES {smiles}: {str(e)}\")\n                features.append(np.zeros(200))\n        \n        logger.info(f\"Successfully extracted features for {len(features)} molecules\")\n        return np.array(features)\n        \n    def _weighted_mae(self, y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        \"\"\"Calculate weighted MAE as per competition metric.\n        \n        Args:\n            y_true: True values\n            y_pred: Predicted values\n            \n        Returns:\n            float: Weighted MAE score\n        \"\"\"\n        logger.info(\"Calculating weighted MAE score...\")\n        \n        # Count number of samples for each property\n        n_p = {col: len(y_true[col].dropna()) for col in y_true.columns}\n        logger.info(f\"Number of samples per property: {n_p}\")\n        \n        # Calculate weights based on sample count and property range\n        weights = {p: (n_p[p]**-0.5)/self.property_ranges[p] for p in self.property_ranges}\n        total_weight = sum(weights.values())\n        norm_weights = {p: weights[p] * len(weights)/total_weight for p in weights}\n        logger.info(f\"Normalized weights: {norm_weights}\")\n        \n        # Calculate weighted errors\n        errors = np.abs(y_true - y_pred)\n        weighted_score = np.mean([errors[p].mean() * norm_weights[p] for p in y_true.columns])\n        \n        logger.info(f\"Final weighted MAE score: {weighted_score:.4f}\")\n        return weighted_score\n        \n    def optimize_xgb(self, X, y, param_space, n_trials=20):\n        \"\"\"\n        Use Optuna to find the best XGBoost hyperparameters for a given property.\n        Args:\n            X: Feature matrix\n            y: Target vector\n            param_space: Dictionary specifying the search space for each hyperparameter\n            n_trials: Number of Optuna trials\n        Returns:\n            dict: Best hyperparameters found\n        \"\"\"\n        logger.info(f\"Starting Optuna optimization for property with {n_trials} trials...\")\n        def objective(trial):\n            params = {}\n            for k, v in param_space.items():\n                if isinstance(v, tuple) and len(v) == 2:\n                    # Float or int range\n                    if all(isinstance(x, int) for x in v):\n                        params[k] = trial.suggest_int(k, v[0], v[1])\n                    else:\n                        params[k] = trial.suggest_float(k, v[0], v[1], log=True if k == 'learning_rate' else False)\n                elif isinstance(v, list):\n                    params[k] = trial.suggest_categorical(k, v)\n            params.update(self.xgb_params)\n            params['objective'] = 'reg:absoluteerror'  # Changed to MAE objective\n            params['random_state'] = 42\n\n            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n            model = xgb.XGBRegressor(**params)\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=20, verbose=False)\n            preds = model.predict(X_valid)\n            return mean_absolute_error(y_valid, preds)\n\n        study = optuna.create_study(direction='minimize')\n        study.optimize(objective, n_trials=n_trials)\n        logger.info(f\"Best params found: {study.best_params}\")\n        return study.best_params\n\n    def fit(self, train_smiles: list, train_targets: pd.DataFrame, optimize=False, param_space=None, n_trials=20) -> None:\n        \"\"\"Train the model on the provided data. Optionally use Optuna for hyperparameter optimization.\n        \n        Args:\n            train_smiles: List of SMILES strings\n            train_targets: DataFrame with target properties\n            optimize: Whether to use Optuna for hyperparameter search\n            param_space: Dictionary specifying the search space for each hyperparameter\n            n_trials: Number of Optuna trials\n        \"\"\"\n        logger.info(\"Starting model training process...\")\n        \n        # Extract features from SMILES strings\n        logger.info(\"Extracting molecular features from SMILES strings using RDKit...\")\n        X = self._extract_features(train_smiles)\n        logger.info(f\"Extracted {X.shape[1]} features from {X.shape[0]} molecules\")\n        \n        # Train separate models for each property\n        for property_name in train_targets.columns:\n            logger.info(f\"\\nTraining model for {property_name}...\")\n            \n            # Get valid indices for this property\n            valid_idx = ~train_targets[property_name].isna()\n            if not valid_idx.any():\n                logger.warning(f\"No valid data for {property_name}, skipping...\")\n                continue\n                \n            X_prop = X[valid_idx]\n            y_prop = train_targets[property_name][valid_idx]\n            logger.info(f\"Training data size for {property_name}: {len(y_prop)} samples\")\n            \n            # Scale features\n            logger.info(\"Scaling features using StandardScaler...\")\n            scaler = StandardScaler()\n            X_scaled = scaler.fit_transform(X_prop)\n            self.scalers[property_name] = scaler\n            \n            # Hyperparameter optimization with Optuna if requested\n            if optimize and param_space is not None:\n                logger.info(f\"Optimizing hyperparameters for {property_name} using Optuna...\")\n                best_params = self.optimize_xgb(X_scaled, y_prop, param_space, n_trials)\n                model_params = best_params.copy()\n                model_params.update(self.xgb_params)\n                model_params['objective'] = 'reg:absoluteerror'\n                model_params['random_state'] = 42\n            else:\n                model_params = dict(\n                    n_estimators=self.n_estimators,\n                    learning_rate=self.learning_rate,\n                    objective='reg:absoluteerror',\n                    random_state=42,\n                    **self.xgb_params,\n                    max_depth=7,\n                    min_child_weight=1,\n                    subsample=0.8,\n                    colsample_bytree=0.8,\n                    gamma=0.1,\n                    reg_alpha=0.1,\n                    reg_lambda=1.0\n                )\n            \n            # Train XGBoost model\n            logger.info(\"Training XGBoost model with parameters: {}\".format(model_params))\n            model = xgb.XGBRegressor(**model_params)\n            logger.info(\"Starting model training with early stopping...\")\n            model.fit(\n                X_scaled, y_prop,\n                eval_set=[(X_scaled, y_prop)],\n                early_stopping_rounds=100,\n                verbose=100\n            )\n            \n            self.models[property_name] = model\n            logger.info(f\"Completed training for {property_name}\")\n            \n    def predict(self, test_smiles: list) -> pd.DataFrame:\n        \"\"\"Make predictions for new SMILES strings.\n        \n        Args:\n            test_smiles: List of SMILES strings to predict\n            \n        Returns:\n            pd.DataFrame: Predictions for all properties\n        \"\"\"\n        logger.info(\"Starting prediction process...\")\n        \n        # Extract features from test SMILES\n        logger.info(\"Extracting features from test molecules using RDKit...\")\n        X = self._extract_features(test_smiles)\n        logger.info(f\"Extracted features for {len(test_smiles)} test molecules\")\n        \n        # Make predictions for each property\n        predictions = {}\n        for property_name, model in self.models.items():\n            logger.info(f\"Making predictions for {property_name}...\")\n            X_scaled = self.scalers[property_name].transform(X)\n            \n            # Create DMatrix for faster prediction\n            dtest = xgb.DMatrix(X_scaled)\n            predictions[property_name] = model.predict(X_scaled)\n            \n        logger.info(\"Completed all predictions\")\n        return pd.DataFrame(predictions)\n\n    def fit_predict_best_model(self, train_smiles, train_targets, test_smiles, best_params):\n        \"\"\"\n        Fit and predict in one shot using the best parameters for all properties.\n        Args:\n            train_smiles: List of SMILES strings for training\n            train_targets: DataFrame with target properties\n            test_smiles: List of SMILES strings for prediction\n            best_params: Dict of best hyperparameters (from Optuna or manual)\n        Returns:\n            pd.DataFrame: Predictions for all properties\n        \"\"\"\n        logger.info(\"Fitting and predicting with best parameters in one shot...\")\n        # Extract features\n        X_train = self._extract_features(train_smiles)\n        X_test = self._extract_features(test_smiles)\n        predictions = {}\n\n        # Create directory for saving models if it doesn't exist\n        os.makedirs('saved_models', exist_ok=True)\n\n        for property_name in train_targets.columns:\n            logger.info(f\"Processing property: {property_name}\")\n            valid_idx = ~train_targets[property_name].isna()\n            if not valid_idx.any():\n                logger.warning(f\"No valid data for {property_name}, skipping...\")\n                continue\n\n            X_prop = X_train[valid_idx]\n            y_prop = train_targets[property_name][valid_idx]\n\n            # Scale features\n            scaler = StandardScaler()\n            X_scaled = scaler.fit_transform(X_prop)\n            X_test_scaled = scaler.transform(X_test)\n            self.scalers[property_name] = scaler\n\n            # Merge best params with default XGBoost params\n            model_params = best_params.copy()\n            model_params.update(self.xgb_params)\n            model_params['objective'] = 'reg:absoluteerror'  # Changed to MAE objective\n            model_params['random_state'] = 42\n\n            logger.info(f\"Training XGBoost model for {property_name} with best parameters: {model_params}\")\n            model = xgb.XGBRegressor(**model_params)\n            model.fit(X_scaled, y_prop, eval_set=[(X_scaled, y_prop)], early_stopping_rounds=100, verbose=100)\n            self.models[property_name] = model\n\n            # Save model and scaler\n            model_path = os.path.join('saved_models', f'{property_name}_model.joblib')\n            scaler_path = os.path.join('saved_models', f'{property_name}_scaler.joblib')\n            joblib.dump(model, model_path)\n            joblib.dump(scaler, scaler_path)\n            logger.info(f\"Saved model and scaler for {property_name}\")\n\n            # Predict\n            predictions[property_name] = model.predict(X_test_scaled)\n\n        logger.info(\"Completed fit and predict for all properties.\")\n        return pd.DataFrame(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:04:50.121215Z","iopub.execute_input":"2025-06-21T12:04:50.121797Z","iopub.status.idle":"2025-06-21T12:04:50.148335Z","shell.execute_reply.started":"2025-06-21T12:04:50.121774Z","shell.execute_reply":"2025-06-21T12:04:50.147669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Use it first to find the best hyperparameters on a state space and then use the best hyperparameters to fit & predict. ","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load data\n    train_smiles, train_targets, test = load_data()\n    \n    #--- Optuna Hyperparameter Optimization Usage ---\n    param_space = {\n        'n_estimators': (2500, 7500),                # Integer range\n        'learning_rate': (0.01, 0.05),              # Float range (log scale for learning_rate)\n        'max_depth': (10, 32),                       # Integer range\n        'subsample': (0.5, 0.9),                    # Float range\n        'colsample_bytree': (0.5, 0.9),             # Float range\n        'gamma': (0, 1.0),                          # Float range\n        'reg_alpha': (0, 1.0),                      # Float range\n        'reg_lambda': (0, 2.0),                     # Float range\n    }\n    n_trials = 100\n    logger.info(\"Initializing and training model with Optuna hyperparameter optimization...\")\n    model = PolymerPredictor()\n    \n    # Dictionary to store best parameters for each property\n    best_params_per_property = {}\n    \n    # Train models for each property\n    for property_name in train_targets.columns:\n        logger.info(f\"\\nOptimizing hyperparameters for {property_name}...\")\n        valid_idx = ~train_targets[property_name].isna()\n        if not valid_idx.any():\n            logger.warning(f\"No valid data for {property_name}, skipping...\")\n            continue\n            \n        X = model._extract_features(train_smiles[valid_idx])\n        y = train_targets[property_name][valid_idx]\n        \n        # Scale features\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n        \n        # Optimize hyperparameters\n        best_params = model.optimize_xgb(X_scaled, y, param_space, n_trials)\n        best_params_per_property[property_name] = best_params\n        \n        # Save scaler\n        scaler_path = f'/kaggle/working/{property_name}_scaler.joblib'\n        joblib.dump(scaler, scaler_path)\n        logger.info(f\"Saved scaler for {property_name} to {scaler_path}\")\n    \n    # Print best parameters for each property\n    logger.info(\"\\nBest hyperparameters for each property:\")\n    best_params_df = pd.DataFrame(best_params_per_property).T\n    print(\"\\nBest Hyperparameters per Property:\")\n    print(best_params_df)\n    \n    # Train final models with best parameters\n    model.fit(\n        train_smiles,\n        train_targets,\n        optimize=False  # We already have the best parameters\n    )\n    \n    # Save models\n    for property_name, model_obj in model.models.items():\n        model_path = f'/kaggle/working/{property_name}_model.joblib'\n        joblib.dump(model_obj, model_path)\n        logger.info(f\"Saved model for {property_name} to {model_path}\")\n    \n    logger.info(\"Making predictions on test set...\")\n    predictions = model.predict(test['SMILES'].values)\n    logger.info(\"Creating submission file...\")\n    submission = pd.DataFrame({\n        'id': test['id'],\n        **predictions\n    })\n    logger.info(\"Saving submission file...\")\n    submission.to_csv('/kaggle/working/submission.csv', index=False)\n    logger.info(\"Process completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T12:05:00.887320Z","iopub.execute_input":"2025-06-21T12:05:00.887921Z","iopub.status.idle":"2025-06-21T12:08:00.256323Z","shell.execute_reply.started":"2025-06-21T12:05:00.887898Z","shell.execute_reply":"2025-06-21T12:08:00.255562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### === Alternative: Load saved models as datasets and predict without training ===","metadata":{}},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     # Load data\n#     train_smiles, train_targets, test = load_data()\n    \n#     # Initialize predictor\n#     model = PolymerPredictor()\n    \n#     # Dictionary to store best parameters for each property\n#     best_params_per_property = {}\n    \n#     # Define the base path for saved models\n#     base_path = '/kaggle/working/'\n    \n#     # Load saved models and extract their parameters\n#     for property_name in train_targets.columns:\n#         model_path = os.path.join(base_path, f'{property_name}_model.joblib')\n#         scaler_path = os.path.join(base_path, f'{property_name}_scaler.joblib')\n        \n#         logger.info(f\"\\nChecking for saved model at: {model_path}\")\n#         logger.info(f\"Checking for saved scaler at: {scaler_path}\")\n        \n#         if os.path.exists(model_path) and os.path.exists(scaler_path):\n#             logger.info(f\"Found saved files for {property_name}\")\n#             try:\n#                 # Load and verify the model\n#                 saved_model = joblib.load(model_path)\n#                 logger.info(f\"Successfully loaded model for {property_name}\")\n                \n#                 # Print raw model parameters for verification\n#                 logger.info(f\"\\nRaw parameters from saved model for {property_name}:\")\n#                 raw_params = saved_model.get_params()\n#                 for param, value in raw_params.items():\n#                     logger.info(f\"{param}: {value}\")\n                \n#                 # Extract specific parameters we need\n#                 best_params = {\n#                     'n_estimators': raw_params.get('n_estimators'),\n#                     'learning_rate': raw_params.get('learning_rate'),\n#                     'max_depth': raw_params.get('max_depth'),\n#                     'subsample': raw_params.get('subsample'),\n#                     'colsample_bytree': raw_params.get('colsample_bytree'),\n#                     'gamma': raw_params.get('gamma'),\n#                     'reg_alpha': raw_params.get('reg_alpha'),\n#                     'reg_lambda': raw_params.get('reg_lambda')\n#                 }\n                \n#                 # Verify the extracted parameters\n#                 logger.info(f\"\\nExtracted parameters for {property_name}:\")\n#                 for param, value in best_params.items():\n#                     logger.info(f\"{param}: {value}\")\n                \n#                 best_params_per_property[property_name] = best_params\n                \n#                 # Load the scaler\n#                 model.scalers[property_name] = joblib.load(scaler_path)\n#                 logger.info(f\"Successfully loaded scaler for {property_name}\")\n                \n#             except Exception as e:\n#                 logger.error(f\"Error loading model or scaler for {property_name}: {str(e)}\")\n#                 continue\n#         else:\n#             logger.warning(f\"Saved files not found for {property_name}\")\n#             if not os.path.exists(model_path):\n#                 logger.warning(f\"Model file missing: {model_path}\")\n#             if not os.path.exists(scaler_path):\n#                 logger.warning(f\"Scaler file missing: {scaler_path}\")\n    \n#     # Print best parameters for each property\n#     logger.info(\"\\nBest hyperparameters for each property:\")\n#     best_params_df = pd.DataFrame(best_params_per_property).T\n#     print(\"\\nBest Hyperparameters per Property:\")\n#     print(best_params_df)\n    \n#     # Verify we have parameters for all properties\n#     missing_properties = set(train_targets.columns) - set(best_params_per_property.keys())\n#     if missing_properties:\n#         logger.warning(f\"Missing parameters for properties: {missing_properties}\")\n#         raise ValueError(\"Not all properties have parameters loaded\")\n    \n#     # Extract features from training data\n#     logger.info(\"Extracting features from training molecules...\")\n#     X_train = model._extract_features(train_smiles)\n    \n#     # Train new models with best parameters\n#     for property_name in train_targets.columns:\n#         if property_name not in best_params_per_property:\n#             continue\n            \n#         logger.info(f\"\\nTraining new model for {property_name} with best parameters...\")\n#         valid_idx = ~train_targets[property_name].isna()\n#         if not valid_idx.any():\n#             logger.warning(f\"No valid data for {property_name}, skipping...\")\n#             continue\n            \n#         X_prop = X_train[valid_idx]\n#         y_prop = train_targets[property_name][valid_idx]\n        \n#         # Scale features\n#         X_scaled = model.scalers[property_name].transform(X_prop)\n        \n#         # Train new model with best parameters\n#         model_params = best_params_per_property[property_name].copy()\n#         model_params.update(model.xgb_params)  # Add GPU/CPU parameters\n#         model_params['objective'] = 'reg:absoluteerror'\n#         model_params['random_state'] = 42\n        \n#         logger.info(f\"Training XGBoost model for {property_name} with parameters: {model_params}\")\n#         new_model = xgb.XGBRegressor(**model_params)\n#         new_model.fit(\n#             X_scaled, y_prop,\n#             eval_set=[(X_scaled, y_prop)],\n#             early_stopping_rounds=50,\n#             verbose=100\n#         )\n#         model.models[property_name] = new_model\n        \n#         # Save the newly trained model\n#         model_path = os.path.join(base_path, f'{property_name}_model_refit.joblib')\n#         joblib.dump(new_model, model_path)\n#         logger.info(f\"Saved refit model for {property_name} to {model_path}\")\n    \n#     # Extract features from test SMILES\n#     logger.info(\"Extracting features from test molecules...\")\n#     X_test = model._extract_features(test['SMILES'].values)\n    \n#     # Make predictions\n#     logger.info(\"Making predictions using refit models...\")\n#     predictions = {}\n#     for property_name, model_obj in model.models.items():\n#         logger.info(f\"Predicting {property_name}...\")\n#         X_test_scaled = model.scalers[property_name].transform(X_test)\n#         preds = model_obj.predict(X_test_scaled)\n#         predictions[property_name] = preds\n        \n#         # Validate predictions\n#         logger.info(f\"Property {property_name} predictions - Shape: {preds.shape}, Min: {preds.min():.2f}, Max: {preds.max():.2f}\")\n    \n#     # Create submission DataFrame\n#     submission = pd.DataFrame({\n#         'id': test['id'].values,  # Ensure we're using values\n#         **predictions\n#     })\n    \n#     # Validate submission format\n#     logger.info(f\"Submission shape: {submission.shape}\")\n#     logger.info(f\"Submission columns: {submission.columns.tolist()}\")\n#     logger.info(f\"Submission dtypes:\\n{submission.dtypes}\")\n    \n#     # Check for any missing values\n#     missing_values = submission.isnull().sum()\n#     if missing_values.any():\n#         logger.warning(f\"Missing values found:\\n{missing_values[missing_values > 0]}\")\n#         # Fill missing values with median of each column\n#         for col in submission.columns:\n#             if submission[col].isnull().any():\n#                 median_val = submission[col].median()\n#                 submission[col] = submission[col].fillna(median_val)\n#                 logger.info(f\"Filled missing values in {col} with median: {median_val}\")\n    \n#     # Ensure all columns are in the correct order\n#     expected_columns = ['id', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n#     submission = submission[expected_columns]\n    \n#     # Save submission\n#     submission.to_csv(os.path.join(base_path, 'submission.csv'), index=False)\n#     logger.info(\"Predictions completed using refit models!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}