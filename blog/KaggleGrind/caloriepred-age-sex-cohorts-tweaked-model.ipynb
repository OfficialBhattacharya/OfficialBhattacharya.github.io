{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Calorie Predictor Series :\n1. Basic Ensemble of XGBoost and CATBoost (Baseline): https://www.kaggle.com/code/digantabhattacharya/caloriepredictor-xgboost-catboost-0-577\n2. XGBoost on Different Cohorts (Slight Improvement): https://www.kaggle.com/code/digantabhattacharya/caloriepredictor-age-sex-cohorts-0-574\n3. Dataset Creation Script with Added Features (Feature Engineering): https://www.kaggle.com/code/digantabhattacharya/caloriepredictor-feature-creation-script-all","metadata":{}},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport time\nimport logging\nimport warnings\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom itertools import combinations\n\n# Third-party imports for data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom scipy.stats import boxcox, skew, spearmanr, pearsonr\nfrom scipy.interpolate import LSQUnivariateSpline\nfrom scipy.io import arff\n\n# Machine learning imports\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold, KFold, \n    GridSearchCV, cross_val_score\n)\nfrom sklearn.preprocessing import (\n    LabelEncoder, StandardScaler, MinMaxScaler,\n    RobustScaler,\n    KBinsDiscretizer, PowerTransformer\n)\nfrom sklearn.ensemble import (\n    RandomForestRegressor, GradientBoostingClassifier,\n    VotingClassifier\n)\nfrom sklearn.metrics import (\n    mean_squared_error, accuracy_score, roc_auc_score,\n    roc_curve, mean_squared_log_error\n)\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model-specific imports\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nimport statsmodels.api as sm\n\n# Deep learning imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utility imports\nimport joblib\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport logging\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom dataclasses import dataclass\nfrom enum import Enum","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:05.711207Z","iopub.execute_input":"2025-05-18T19:17:05.711383Z","iopub.status.idle":"2025-05-18T19:17:24.755448Z","shell.execute_reply.started":"2025-05-18T19:17:05.711366Z","shell.execute_reply":"2025-05-18T19:17:24.754855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:24.756135Z","iopub.execute_input":"2025-05-18T19:17:24.756596Z","iopub.status.idle":"2025-05-18T19:17:24.760839Z","shell.execute_reply.started":"2025-05-18T19:17:24.756576Z","shell.execute_reply":"2025-05-18T19:17:24.760142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data():\n    \"\"\"\n    Load training, testing, and submission data.\n    \n    Returns:\n        tuple: (train_df, test_df, submission_df)\n    \"\"\"\n    logger.info(\"Loading data...\")\n    train = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\n    test = pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")\n    submission = pd.read_csv(\"/kaggle/input/playground-series-s5e5/sample_submission.csv\")\n    print(\"Data Loading Done!\")\n    print(\"Training Data Snap:\")\n    print(train.head(5))\n    print(\"Test Data Snap:\")\n    print(test.head(5))\n    print(\"Submission Format Snap:\")\n    print(submission.head(5))\n    return train, test, submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:24.762397Z","iopub.execute_input":"2025-05-18T19:17:24.762651Z","iopub.status.idle":"2025-05-18T19:17:24.785206Z","shell.execute_reply.started":"2025-05-18T19:17:24.762627Z","shell.execute_reply":"2025-05-18T19:17:24.784710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ntrain_df, test_df, submission_df = load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:24.785759Z","iopub.execute_input":"2025-05-18T19:17:24.785939Z","iopub.status.idle":"2025-05-18T19:17:26.145117Z","shell.execute_reply.started":"2025-05-18T19:17:24.785901Z","shell.execute_reply":"2025-05-18T19:17:26.144281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def intersection_of_lists(list1, list2):\n    return list(set(list1) & set(list2))\n\n\ndef difference_of_lists(list1, list2):\n    return [item for item in list1 if item not in list2]\n\n\ndef remove_single_unique_or_all_nans(df):\n    removed_columns = []\n    for column in df.columns:\n        if df[column].nunique() <= 1 or df[column].isna().all():\n            removed_columns.append(column)\n            df = df.drop(columns=[column])\n    print(f\"Removed columns due to all NaN or only 1 unique value: {removed_columns}\")\n    return df\n\n\ndef columns_with_missing_values(df):\n    missing_cols = [col for col in df.columns if df[col].isna().values.any()]\n    print(f\"Missing data columns: {missing_cols}\")\n    return missing_cols\n\n\ndef columns_with_more_than_X_percent_unique(df, colNames, perc):\n    total_rows = len(df)\n    threshold = total_rows * 0.01 * perc  \n    cols_with_high_uniques = [col for col in colNames if df[col].nunique() > threshold]\n    print(f\"Columns with high uniques , >= {perc} %  of number of rows in the data: {cols_with_high_uniques}\")\n    return cols_with_high_uniques\n\n\ndef get_numeric_and_non_numeric_columns(df):\n        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n        print(f\"Numeric columns: {numeric_cols}\")\n        print(f\"Non-numeric columns: {non_numeric_cols}\")\n        return numeric_cols, non_numeric_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:26.145929Z","iopub.execute_input":"2025-05-18T19:17:26.146256Z","iopub.status.idle":"2025-05-18T19:17:26.153129Z","shell.execute_reply.started":"2025-05-18T19:17:26.146222Z","shell.execute_reply":"2025-05-18T19:17:26.152551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Target_Col = ['Calories']\nIdentifier_Cols = ['id']\nX_Cols = [\"Age\", \"Height\", \"Weight\", \"Duration\", \"Heart_Rate\", \"Body_Temp\", 'Sex']\ndata = train_df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:26.153858Z","iopub.execute_input":"2025-05-18T19:17:26.154120Z","iopub.status.idle":"2025-05-18T19:17:26.199484Z","shell.execute_reply.started":"2025-05-18T19:17:26.154098Z","shell.execute_reply":"2025-05-18T19:17:26.198873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Numeric_Cols, Non_Numeric_Cols = get_numeric_and_non_numeric_columns(data[X_Cols])\nMissingData_Cols = columns_with_missing_values(data[X_Cols])\nGreaterThanTENpercUniQ_Cols = columns_with_more_than_X_percent_unique(data, Numeric_Cols, 75)\nGreaterThanEIGHTpercUniQ_Cols = columns_with_more_than_X_percent_unique(data, Numeric_Cols, 50)\nGreaterThanFIVEpercUniQ_Cols = columns_with_more_than_X_percent_unique(data, Numeric_Cols, 25)\nGreaterThanONEpercUniQ_Cols = columns_with_more_than_X_percent_unique(data, Numeric_Cols, 1)\nLessUniqueNA_Cols = intersection_of_lists(MissingData_Cols, difference_of_lists(GreaterThanONEpercUniQ_Cols, GreaterThanFIVEpercUniQ_Cols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:26.200106Z","iopub.execute_input":"2025-05-18T19:17:26.200297Z","iopub.status.idle":"2025-05-18T19:17:26.557020Z","shell.execute_reply.started":"2025-05-18T19:17:26.200282Z","shell.execute_reply":"2025-05-18T19:17:26.556206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass FeatureType(Enum):\n    BASIC = \"basic\"\n    POLYNOMIAL = \"polynomial\"\n    PHYSIOLOGICAL = \"physiological\"\n    INTERACTION = \"interaction\"\n    SPECIFIC_INTERACTION = \"specific_interaction\"\n    STATISTICAL = \"statistical\"\n    DERIVED = \"derived\"\n\n@dataclass\nclass FeatureConfig:\n    \"\"\"Configuration for feature generation\"\"\"\n    feature_types: List[FeatureType]\n    poly_features: Optional[List[str]] = None\n    interaction_features: Optional[List[str]] = None\n    specific_interactions: Optional[List[List[str]]] = None\n    statistical_features: Optional[List[str]] = None\n\nclass FeatureManager:\n    \"\"\"Manages feature generation and tracking\"\"\"\n    def __init__(self):\n        self.feature_pipelines: Dict[str, Dict] = {}\n        self.datasets: Dict[str, pd.DataFrame] = {}\n        \n    def create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Create basic features like BMI and Intensity\"\"\"\n        df_processed = df.copy()\n        \n        # BMI\n        df_processed['BMI'] = df_processed['Weight'] / (df_processed['Height'] / 100) ** 2\n        \n        # Intensity\n        df_processed['Intensity'] = df_processed['Heart_Rate'] / df_processed['Duration']\n        \n        return df_processed\n    \n    def create_polynomial_features(self, df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n        \"\"\"Create polynomial features from specified columns\"\"\"\n        df_processed = df.copy()\n        \n        for i in range(len(features)):\n            for j in range(i+1, len(features)):\n                feat1, feat2 = features[i], features[j]\n                \n                # Multiplication\n                df_processed[f'{feat1}_x_{feat2}'] = df_processed[feat1] * df_processed[feat2]\n                \n                # Division (avoid division by zero)\n                df_processed[f'{feat1}_div_{feat2}'] = df_processed[feat1] / (df_processed[feat2] + 1e-6)\n                \n                # Square of each feature\n                df_processed[f'{feat1}_squared'] = df_processed[feat1] ** 2\n        \n        return df_processed\n    \n    def create_physiological_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Create advanced physiological features\"\"\"\n        df_processed = df.copy()\n        \n        # Basal Metabolic Rate (BMR)\n        df_processed['BMR'] = df_processed['Weight'] / ((df_processed['Height'] / 100) ** 2)\n        \n        # Metabolic Efficiency Index\n        df_processed['Metabolic_Efficiency'] = df_processed['BMR'] * (df_processed['Heart_Rate'] / df_processed['BMR'].median())\n        \n        # Cardiovascular Stress\n        df_processed['Cardio_Stress'] = (df_processed['Heart_Rate'] / (220 - df_processed['Age'])) * df_processed['Duration']\n        \n        # Thermic Effect Ratio\n        df_processed['Thermic_Effect'] = (df_processed['Body_Temp'] * 100) / (df_processed['Weight'] ** 0.5)\n        \n        # Power Output Estimate\n        df_processed['Power_Output'] = df_processed['Weight'] * df_processed['Duration'] * (df_processed['Heart_Rate'] / 1000)\n        \n        return df_processed\n    \n    def create_interaction_features(self, df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n        \"\"\"Create interaction features between specified columns\"\"\"\n        df_processed = df.copy()\n        \n        # Duration-based features\n        durations = sorted(df_processed['Duration'].unique())\n        for dur in durations:\n            df_processed[f'HR_Dur_{int(dur)}'] = np.where(df_processed['Duration'] == dur, df_processed['Heart_Rate'], 0)\n            df_processed[f'Temp_Dur_{int(dur)}'] = np.where(df_processed['Duration'] == dur, df_processed['Body_Temp'], 0)\n        \n        # Age-based features\n        ages = sorted(df_processed['Age'].unique())\n        for age in ages:\n            df_processed[f'HR_Age_{int(age)}'] = np.where(df_processed['Age'] == age, df_processed['Heart_Rate'], 0)\n            df_processed[f'Temp_Age_{int(age)}'] = np.where(df_processed['Age'] == age, df_processed['Body_Temp'], 0)\n        \n        return df_processed\n\n    def create_specific_interactions(self, df: pd.DataFrame, interaction_pairs: List[List[str]]) -> pd.DataFrame:\n        \"\"\"\n        Create specific interaction features based on provided pairs of features\n        \n        Args:\n            df: Input dataframe\n            interaction_pairs: List of feature pairs to create interactions for\n                             Each pair should be a list of two feature names\n        \n        Returns:\n            DataFrame with added interaction features\n        \"\"\"\n        df_processed = df.copy()\n        \n        for pair in interaction_pairs:\n            if len(pair) != 2:\n                logger.warning(f\"Skipping invalid interaction pair: {pair}. Must contain exactly 2 features.\")\n                continue\n                \n            feat1, feat2 = pair\n            if feat1 not in df.columns or feat2 not in df.columns:\n                logger.warning(f\"Skipping interaction pair {pair}. One or both features not found in dataset.\")\n                continue\n            \n            # Create interaction name\n            interaction_name = f\"{feat1}_interact_{feat2}\"\n            \n            # Create interaction (multiplication)\n            df_processed[interaction_name] = df_processed[feat1] * df_processed[feat2]\n            \n            # Create ratio interaction (avoid division by zero)\n            ratio_name = f\"{feat1}_ratio_{feat2}\"\n            df_processed[ratio_name] = df_processed[feat1] / (df_processed[feat2] + 1e-6)\n            \n            logger.info(f\"Created interactions for pair: {pair}\")\n        \n        return df_processed\n    \n    def create_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame, config: FeatureConfig) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Create features based on specified configuration\n        \n        Args:\n            train_df: Training dataframe\n            test_df: Test dataframe\n            config: Feature configuration object\n            \n        Returns:\n            Tuple of processed training and test dataframes\n        \"\"\"\n        logger.info(\"Starting feature engineering with specified configuration...\")\n        \n        # Create copies\n        train_processed = train_df.copy()\n        test_processed = test_df.copy()\n        \n        # Basic preprocessing\n        train_processed['Sex'] = train_processed['Sex'].map({'male': 1, 'female': 0})\n        test_processed['Sex'] = test_processed['Sex'].map({'male': 1, 'female': 0})\n        \n        # Apply feature generation based on configuration\n        if FeatureType.BASIC in config.feature_types:\n            train_processed = self.create_basic_features(train_processed)\n            test_processed = self.create_basic_features(test_processed)\n        \n        if FeatureType.POLYNOMIAL in config.feature_types and config.poly_features:\n            train_processed = self.create_polynomial_features(train_processed, config.poly_features)\n            test_processed = self.create_polynomial_features(test_processed, config.poly_features)\n        \n        if FeatureType.PHYSIOLOGICAL in config.feature_types:\n            train_processed = self.create_physiological_features(train_processed)\n            test_processed = self.create_physiological_features(test_processed)\n        \n        if FeatureType.INTERACTION in config.feature_types and config.interaction_features:\n            train_processed = self.create_interaction_features(train_processed, config.interaction_features)\n            test_processed = self.create_interaction_features(test_processed, config.interaction_features)\n            \n        if FeatureType.SPECIFIC_INTERACTION in config.feature_types and config.specific_interactions:\n            train_processed = self.create_specific_interactions(train_processed, config.specific_interactions)\n            test_processed = self.create_specific_interactions(test_processed, config.specific_interactions)\n        \n        return train_processed, test_processed\n    \n    def create_final_dataset(self, df: pd.DataFrame, selected_features: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Create final dataset with selected features and return both selected and complete feature datasets\n        \n        Args:\n            df: Input dataframe\n            selected_features: List of feature names to include\n            \n        Returns:\n            Tuple of (selected_features_df, complete_features_df)\n            - selected_features_df: DataFrame with only selected features\n            - complete_features_df: DataFrame with all available features\n        \"\"\"\n        available_features = [col for col in df.columns if col in selected_features]\n        if not available_features:\n            raise ValueError(\"None of the selected features are available in the dataset\")\n        \n        selected_df = df[available_features]\n        complete_df = df.copy()\n        \n        logger.info(f\"Created final dataset with {len(available_features)} selected features out of {len(df.columns)} total features\")\n        \n        return selected_df, complete_df\n    \n    def track_pipeline(self, pipeline_name: str, config: FeatureConfig, train_df: pd.DataFrame, test_df: pd.DataFrame):\n        \"\"\"\n        Track feature pipeline and datasets\n        \n        Args:\n            pipeline_name: Name of the pipeline\n            config: Feature configuration used\n            train_df: Training dataframe\n            test_df: Test dataframe\n        \"\"\"\n        self.feature_pipelines[pipeline_name] = {\n            'config': config,\n            'feature_count': len(train_df.columns),\n            'feature_names': list(train_df.columns)\n        }\n        \n        self.datasets[f\"{pipeline_name}_train\"] = train_df\n        self.datasets[f\"{pipeline_name}_test\"] = test_df\n        \n        logger.info(f\"Pipeline '{pipeline_name}' tracked with {len(train_df.columns)} features\")\n\ndef process_features_with_config(\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    feature_types: List[str],\n    poly_features: Optional[List[str]] = None,\n    interaction_features: Optional[List[str]] = None,\n    specific_interactions: Optional[List[List[str]]] = None\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Process features based on provided configuration\n    \n    Args:\n        train_df: Training dataframe\n        test_df: Test dataframe\n        feature_types: List of feature types to include (e.g., ['basic', 'polynomial', 'physiological'])\n        poly_features: List of features for polynomial combinations\n        interaction_features: List of features for general interactions\n        specific_interactions: List of feature pairs for specific interactions\n        \n    Returns:\n        Tuple of (processed_train_df, processed_test_df)\n    \"\"\"\n    # Convert string feature types to enum\n    selected_feature_types = [FeatureType(ft.lower()) for ft in feature_types]\n    \n    # Create feature configuration\n    config = FeatureConfig(\n        feature_types=selected_feature_types,\n        poly_features=poly_features,\n        interaction_features=interaction_features,\n        specific_interactions=specific_interactions\n    )\n    \n    # Create feature manager and process features\n    feature_manager = FeatureManager()\n    train_processed, test_processed = feature_manager.create_features(train_df, test_df, config)\n    \n    # Track the pipeline\n    feature_manager.track_pipeline(\"configured_pipeline\", config, train_processed, test_processed)\n    \n    print(f\"\\nCreated processed datasets:\")\n    print(f\"Training data shape: {train_processed.shape}\")\n    print(f\"Test data shape: {test_processed.shape}\")\n    \n    return train_processed, test_processed\n\n\ndef train_xgboost_parallel(X, y, X_test, base_params=None):\n    \"\"\"\n    Train XGBoost model with parallel cross-validation across multiple GPUs if available,\n    otherwise falls back to CPU training.\n    \n    Args:\n        X (pd.DataFrame): Training features\n        y (pd.Series): Target variable\n        X_test (pd.DataFrame): Test features\n        base_params (dict, optional): Base XGBoost parameters\n        \n    Returns:\n        tuple: (predictions, out-of-fold predictions, scores, model, total_time)\n    \"\"\"\n    logger.info(\"Starting parallel XGBoost training...\")\n    print(\"\\n=== Starting Parallel XGBoost Training ===\")\n    start_time = time.time()\n    \n    # Prepare data\n    logger.info(\"Preparing data for XGBoost...\")\n    print(\"Preparing data for XGBoost...\")\n    \n    # Ensure feature alignment\n    logger.info(\"Aligning features between train and test...\")\n    print(\"Aligning features between train and test...\")\n    common_features = list(set(X.columns) & set(X_test.columns))\n    X_xgb = X[common_features].copy()\n    X_test_xgb = X_test[common_features].copy()\n    \n    # Convert categorical features\n    X_xgb['Sex'] = X_xgb['Sex'].astype(int)\n    X_test_xgb['Sex'] = X_test_xgb['Sex'].astype(int)\n\n    # Initialize prediction arrays\n    xgb_oof = np.zeros(len(X))\n    xgb_preds = np.zeros(len(X_test))\n    xgb_scores = []\n\n    # Check for GPU availability\n    try:\n        n_gpus = torch.cuda.device_count()\n        if n_gpus > 0:\n            logger.info(f\"Found {n_gpus} GPUs\")\n            print(f\"Found {n_gpus} GPUs\")\n            device = \"cuda\"\n            tree_method = \"hist\"\n        else:\n            logger.info(\"No GPUs found, using CPU\")\n            print(\"No GPUs found, using CPU\")\n            device = \"cpu\"\n            tree_method = \"hist\"\n    except:\n        logger.info(\"GPU check failed, using CPU\")\n        print(\"GPU check failed, using CPU\")\n        device = \"cpu\"\n        tree_method = \"hist\"\n\n    # Use provided parameters or defaults\n    params = base_params or {\n        'max_depth': 9,\n        'colsample_bytree': 0.7,\n        'subsample': 0.9,\n        'n_estimators': 3000,\n        'learning_rate': 0.01,\n        'gamma': 0.01,\n        'max_delta_step': 2,\n        'eval_metric': 'rmse',\n        'enable_categorical': False,\n        'random_state': 42,\n        'early_stopping_rounds': 100,\n        'tree_method': tree_method,\n        'device': device,\n        'sampling_method': 'gradient_based',\n        'max_bin': 256,\n        'grow_policy': 'lossguide'\n    }\n\n    # Cross-validation\n    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n    total_folds = kf.n_splits\n    \n    # Store all splits\n    splits = list(kf.split(X_xgb))\n    \n    logger.info(f\"Starting {total_folds}-fold cross-validation in parallel...\")\n    print(f\"\\nStarting {total_folds}-fold cross-validation in parallel...\")\n    logger.info(f\"Training with {len(common_features)} features: {', '.join(common_features)}\")\n    print(f\"Training with {len(common_features)} features\")\n\n    def train_fold(fold_idx, train_idx, val_idx, device_id):\n        \"\"\"Train a single fold on specified device\"\"\"\n        fold_start_time = time.time()\n        logger.info(f\"\\nFold {fold_idx}/{total_folds} - Starting training on {device}...\")\n        print(f\"\\nFold {fold_idx}/{total_folds} - Starting training on {device}...\")\n        \n        # Create fold-specific parameters\n        fold_params = params.copy()\n        if device == \"cuda\":\n            fold_params['device'] = f\"cuda:{device_id}\"\n        \n        # Create and train model\n        model = XGBRegressor(**fold_params)\n        \n        # Train model with progress logging\n        model.fit(\n            X_xgb.iloc[train_idx], \n            y.iloc[train_idx],\n            eval_set=[(X_xgb.iloc[val_idx], y.iloc[val_idx])],\n            verbose=100\n        )\n        \n        # Make predictions\n        logger.info(f\"Fold {fold_idx} - Making predictions...\")\n        print(f\"Fold {fold_idx} - Making predictions...\")\n        fold_oof = model.predict(X_xgb.iloc[val_idx])\n        fold_test_preds = model.predict(X_test_xgb)\n        \n        # Calculate fold score\n        fold_score = np.sqrt(mean_squared_log_error(\n            np.expm1(y.iloc[val_idx]), \n            np.expm1(fold_oof)\n        ))\n        \n        # Calculate fold timing\n        fold_time = time.time() - fold_start_time\n        \n        logger.info(f\"Fold {fold_idx} completed on {device}:\")\n        logger.info(f\"  - RMSLE Score: {fold_score:.5f}\")\n        logger.info(f\"  - Time taken: {fold_time:.2f} seconds\")\n        print(f\"\\nFold {fold_idx} completed on {device}:\")\n        print(f\"  - RMSLE Score: {fold_score:.5f}\")\n        print(f\"  - Time taken: {fold_time:.2f} seconds\")\n        \n        return fold_idx, fold_oof, fold_test_preds, fold_score, model, fold_time, val_idx\n\n    # Create process pool for parallel execution\n    max_workers = n_gpus if device == \"cuda\" else 1\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all folds for parallel execution\n        future_to_fold = {\n            executor.submit(\n                train_fold, \n                fold_idx + 1, \n                train_idx, \n                val_idx, \n                fold_idx % max_workers\n            ): fold_idx \n            for fold_idx, (train_idx, val_idx) in enumerate(splits)\n        }\n        \n        # Collect results as they complete\n        fold_times = []\n        for future in as_completed(future_to_fold):\n            fold_idx, fold_oof, fold_test_preds, fold_score, model, fold_time, val_idx = future.result()\n            xgb_oof[val_idx] = fold_oof\n            xgb_preds += fold_test_preds / total_folds\n            xgb_scores.append(fold_score)\n            fold_times.append(fold_time)\n            \n            # Estimate remaining time\n            if len(fold_times) < total_folds:\n                avg_fold_time = sum(fold_times) / len(fold_times)\n                remaining_folds = total_folds - len(fold_times)\n                estimated_time = avg_fold_time * remaining_folds\n                logger.info(f\"Estimated time remaining: {estimated_time/60:.1f} minutes\")\n                print(f\"Estimated time remaining: {estimated_time/60:.1f} minutes\")\n    \n    # Calculate and log final metrics\n    total_time = time.time() - start_time\n    mean_score = np.mean(xgb_scores)\n    std_score = np.std(xgb_scores)\n    \n    logger.info(\"\\nParallel XGBoost Training Summary:\")\n    logger.info(f\"  - Mean RMSLE: {mean_score:.5f} ± {std_score:.5f}\")\n    logger.info(f\"  - Total training time: {total_time/60:.1f} minutes\")\n    logger.info(f\"  - Average fold time: {total_time/total_folds:.1f} seconds\")\n    print(\"\\n=== Parallel XGBoost Training Summary ===\")\n    print(f\"  - Mean RMSLE: {mean_score:.5f} ± {std_score:.5f}\")\n    print(f\"  - Total training time: {total_time/60:.1f} minutes\")\n    print(f\"  - Average fold time: {total_time/total_folds:.1f} seconds\")\n    \n    return xgb_preds, xgb_oof, xgb_scores, model, total_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:26.557998Z","iopub.execute_input":"2025-05-18T19:17:26.558226Z","iopub.status.idle":"2025-05-18T19:17:26.595723Z","shell.execute_reply.started":"2025-05-18T19:17:26.558207Z","shell.execute_reply":"2025-05-18T19:17:26.594958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define feature configuration\nfeature_types = ['basic', 'polynomial', 'physiological', 'interaction', 'specific_interaction']\n\npoly_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'BMI']\n\ninteraction_features = ['Duration', 'Age', 'Heart_Rate', 'Body_Temp']\n\nspecific_interactions = [\n    ['Heart_Rate', 'Duration'],\n    ['Body_Temp', 'Weight'],\n    ['Age', 'BMI']\n]\n\n# Process features\ntrain_processed, test_processed = process_features_with_config(\n    train_df=train_df,\n    test_df=test_df,\n    feature_types=feature_types,\n    poly_features=poly_features,\n    interaction_features=interaction_features,\n    specific_interactions=specific_interactions\n)\n# Prepare data for modeling\nX = train_processed.drop(columns=['id','Calories'])\ny = np.log1p(train_processed['Calories'])\nX_test = test_processed.drop(columns=['id'])\ntest_ids = test_processed['id']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:26.598229Z","iopub.execute_input":"2025-05-18T19:17:26.598442Z","iopub.status.idle":"2025-05-18T19:17:31.715765Z","shell.execute_reply.started":"2025-05-18T19:17:26.598428Z","shell.execute_reply":"2025-05-18T19:17:31.714959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_gender_specific_models(X, y, X_test, test_ids, male_params=None, female_params=None):\n    \"\"\"\n    Train separate XGBoost models for males and females and combine predictions.\n    \n    Args:\n        X (pd.DataFrame): Training features\n        y (pd.Series): Target variable\n        X_test (pd.DataFrame): Test features\n        test_ids (pd.Series): Test IDs for final submission\n        male_params (dict, optional): XGBoost parameters for male model\n        female_params (dict, optional): XGBoost parameters for female model\n        \n    Returns:\n        tuple: (combined_predictions, male_model, female_model, total_time)\n    \"\"\"\n    logger.info(\"Starting gender-specific model training...\")\n    print(\"\\n=== Starting Gender-Specific Model Training ===\")\n    start_time = time.time()\n    \n    # Default parameters if none provided\n    default_params = {\n        'max_depth': 9,\n        'colsample_bytree': 0.7,\n        'subsample': 0.9,\n        'n_estimators': 3000,\n        'learning_rate': 0.01,\n        'gamma': 0.01,\n        'max_delta_step': 2,\n        'eval_metric': 'rmse',\n        'enable_categorical': False,\n        'random_state': 42,\n        'early_stopping_rounds': 100,\n        'tree_method': 'hist',\n        'sampling_method': 'gradient_based',\n        'max_bin': 256,\n        'grow_policy': 'lossguide'\n    }\n    \n    # Use provided parameters or defaults\n    male_params = male_params or default_params.copy()\n    female_params = female_params or default_params.copy()\n    \n    # Split data by gender\n    male_mask = X['Sex'] == 1\n    female_mask = X['Sex'] == 0\n    \n    X_male = X[male_mask].copy()\n    y_male = y[male_mask].copy()\n    X_female = X[female_mask].copy()\n    y_female = y[female_mask].copy()\n    \n    # Split test data\n    X_test_male = X_test[X_test['Sex'] == 1].copy()\n    X_test_female = X_test[X_test['Sex'] == 0].copy()\n    \n    logger.info(f\"Male training samples: {len(X_male)}\")\n    logger.info(f\"Female training samples: {len(X_female)}\")\n    print(f\"Male training samples: {len(X_male)}\")\n    print(f\"Female training samples: {len(X_female)}\")\n    \n    # Train male model\n    logger.info(\"\\nTraining male model...\")\n    print(\"\\nTraining male model...\")\n    male_preds, male_oof, male_scores, male_model, male_time = train_xgboost_parallel(\n        X_male, y_male, X_test_male, base_params=male_params\n    )\n    \n    # Train female model\n    logger.info(\"\\nTraining female model...\")\n    print(\"\\nTraining female model...\")\n    female_preds, female_oof, female_scores, female_model, female_time = train_xgboost_parallel(\n        X_female, y_female, X_test_female, base_params=female_params\n    )\n    \n    # Combine predictions\n    combined_predictions = pd.DataFrame({\n        'id': test_ids,\n        'Calories': np.zeros(len(test_ids))\n    })\n    \n    # Fill predictions based on gender\n    male_test_mask = X_test['Sex'] == 1\n    female_test_mask = X_test['Sex'] == 0\n    \n    combined_predictions.loc[male_test_mask, 'Calories'] = np.expm1(male_preds)\n    combined_predictions.loc[female_test_mask, 'Calories'] = np.expm1(female_preds)\n    \n    # Calculate and log final metrics\n    total_time = time.time() - start_time\n    male_mean_score = np.mean(male_scores)\n    female_mean_score = np.mean(female_scores)\n    \n    logger.info(\"\\nGender-Specific Model Training Summary:\")\n    logger.info(f\"  - Male Model Mean RMSLE: {male_mean_score:.5f}\")\n    logger.info(f\"  - Female Model Mean RMSLE: {female_mean_score:.5f}\")\n    logger.info(f\"  - Total training time: {total_time/60:.1f} minutes\")\n    print(\"\\n=== Gender-Specific Model Training Summary ===\")\n    print(f\"  - Male Model Mean RMSLE: {male_mean_score:.5f}\")\n    print(f\"  - Female Model Mean RMSLE: {female_mean_score:.5f}\")\n    print(f\"  - Total training time: {total_time/60:.1f} minutes\")\n    \n    return combined_predictions, male_model, female_model, total_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:31.716565Z","iopub.execute_input":"2025-05-18T19:17:31.716825Z","iopub.status.idle":"2025-05-18T19:17:31.726534Z","shell.execute_reply.started":"2025-05-18T19:17:31.716806Z","shell.execute_reply":"2025-05-18T19:17:31.725671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# male_params = {\n#     'max_depth': 8,\n#     'n_estimators': 2500,\n#     'learning_rate': 0.015,\n#     'subsample': 0.8,\n#     'colsample_bytree': 0.8,\n#     'gamma': 0.02,\n#     'tree_method': 'hist',\n#     'random_state': 42\n# }\n\n# female_params = {\n#     'max_depth': 10,\n#     'n_estimators': 3000,\n#     'learning_rate': 0.01,\n#     'subsample': 0.9,\n#     'colsample_bytree': 0.7,\n#     'gamma': 0.01,\n#     'tree_method': 'hist',\n#     'random_state': 42\n# }\n\n# # Train gender-specific models\n# predictions, male_model, female_model, total_time = train_gender_specific_models(\n#     X, y, X_test, test_ids,\n#     male_params=male_params,\n#     female_params=female_params\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_age_gender_specific_models(X, y, X_test, test_ids, group_params=None):\n    \"\"\"\n    Train separate XGBoost models for different age and gender groups.\n    Age groups:\n    - Male: 18-30, 31-45, 46+\n    - Female: 18-35, 36+\n    \n    Args:\n        X (pd.DataFrame): Training features\n        y (pd.Series): Target variable\n        X_test (pd.DataFrame): Test features\n        test_ids (pd.Series): Test IDs for final submission\n        group_params (dict, optional): Dictionary of XGBoost parameters for each group\n        \n    Returns:\n        tuple: (combined_predictions, models_dict, total_time)\n    \"\"\"\n    logger.info(\"Starting age and gender-specific model training...\")\n    print(\"\\n=== Starting Age and Gender-Specific Model Training ===\")\n    start_time = time.time()\n    \n    # Default parameters if none provided\n    default_params = {\n        'max_depth': 9,\n        'colsample_bytree': 0.7,\n        'subsample': 0.9,\n        'n_estimators': 3000,\n        'learning_rate': 0.01,\n        'gamma': 0.01,\n        'max_delta_step': 2,\n        'eval_metric': 'rmse',\n        'enable_categorical': False,\n        'random_state': 42,\n        'early_stopping_rounds': 100,\n        'tree_method': 'hist',\n        'sampling_method': 'gradient_based',\n        'max_bin': 256,\n        'grow_policy': 'lossguide'\n    }\n    \n    # Define age groups\n    age_groups = {\n        'male_18_30': {'gender': 1, 'min_age': 18, 'max_age': 30},\n        'male_31_45': {'gender': 1, 'min_age': 31, 'max_age': 45},\n        'male_46_plus': {'gender': 1, 'min_age': 46, 'max_age': 100},\n        'female_18_35': {'gender': 0, 'min_age': 18, 'max_age': 35},\n        'female_36_plus': {'gender': 0, 'min_age': 36, 'max_age': 100}\n    }\n    \n    # Initialize storage for models and predictions\n    models_dict = {}\n    test_predictions = {}\n    \n    # Train models for each group\n    for group_name, criteria in age_groups.items():\n        logger.info(f\"\\nTraining model for {group_name}...\")\n        print(f\"\\nTraining model for {group_name}...\")\n        \n        # Get parameters for this group\n        group_specific_params = group_params.get(group_name, default_params.copy()) if group_params else default_params.copy()\n        \n        # Create masks for training data\n        train_mask = (\n            (X['Sex'] == criteria['gender']) & \n            (X['Age'] >= criteria['min_age']) & \n            (X['Age'] <= criteria['max_age'])\n        )\n        \n        # Create masks for test data\n        test_mask = (\n            (X_test['Sex'] == criteria['gender']) & \n            (X_test['Age'] >= criteria['min_age']) & \n            (X_test['Age'] <= criteria['max_age'])\n        )\n        \n        # Get group data\n        X_group = X[train_mask].copy()\n        y_group = y[train_mask].copy()\n        X_test_group = X_test[test_mask].copy()\n        \n        # Log group sizes\n        logger.info(f\"{group_name} training samples: {len(X_group)}\")\n        logger.info(f\"{group_name} test samples: {len(X_test_group)}\")\n        print(f\"{group_name} training samples: {len(X_group)}\")\n        print(f\"{group_name} test samples: {len(X_test_group)}\")\n        \n        # Skip if no training samples\n        if len(X_group) == 0:\n            logger.warning(f\"No training samples for {group_name}, skipping...\")\n            print(f\"No training samples for {group_name}, skipping...\")\n            continue\n        \n        # Train model for this group\n        group_preds, group_oof, group_scores, group_model, group_time = train_xgboost_parallel(\n            X_group, y_group, X_test_group, base_params=group_specific_params\n        )\n        \n        # Store model and predictions\n        models_dict[group_name] = {\n            'model': group_model,\n            'scores': group_scores,\n            'mean_score': np.mean(group_scores),\n            'time': group_time,\n            'params': group_specific_params\n        }\n        \n        # Store test predictions with their indices\n        test_predictions[group_name] = {\n            'predictions': np.expm1(group_preds),\n            'test_indices': X_test[test_mask].index\n        }\n    \n    # Combine predictions\n    combined_predictions = pd.DataFrame({\n        'id': test_ids,\n        'Calories': np.zeros(len(test_ids))\n    })\n    \n    # Fill predictions for each group\n    for group_name, pred_data in test_predictions.items():\n        combined_predictions.loc[pred_data['test_indices'], 'Calories'] = pred_data['predictions']\n    \n    # Calculate and log final metrics\n    total_time = time.time() - start_time\n    \n    logger.info(\"\\nAge and Gender-Specific Model Training Summary:\")\n    print(\"\\n=== Age and Gender-Specific Model Training Summary ===\")\n    \n    for group_name, model_data in models_dict.items():\n        logger.info(f\"  - {group_name} Mean RMSLE: {model_data['mean_score']:.5f}\")\n        print(f\"  - {group_name} Mean RMSLE: {model_data['mean_score']:.5f}\")\n    \n    logger.info(f\"  - Total training time: {total_time/60:.1f} minutes\")\n    print(f\"  - Total training time: {total_time/60:.1f} minutes\")\n    \n    return combined_predictions, models_dict, total_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:31.727652Z","iopub.execute_input":"2025-05-18T19:17:31.727896Z","iopub.status.idle":"2025-05-18T19:17:31.759841Z","shell.execute_reply.started":"2025-05-18T19:17:31.727881Z","shell.execute_reply":"2025-05-18T19:17:31.759137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"group_params = {\n    'male_18_30': {\n        'max_depth': 11,\n        'n_estimators': 7500,\n        'learning_rate': 0.0001,\n        'subsample': 0.6,\n        'colsample_bytree': 0.65\n    },\n    'male_31_45': {\n        'max_depth': 10,\n        'n_estimators': 4500,\n        'learning_rate': 0.0009,\n        'subsample': 0.7,\n        'colsample_bytree': 0.85\n    },\n    'male_46_plus': {\n        'max_depth': 10,\n        'n_estimators': 4500,\n        'learning_rate': 0.0009,\n        'subsample': 0.7,\n        'colsample_bytree': 0.85\n    },\n    'female_18_35': {\n        'max_depth': 10,\n        'n_estimators': 4500,\n        'learning_rate': 0.0009,\n        'subsample': 0.7,\n        'colsample_bytree': 0.85\n    },\n    'female_36_plus': {\n        'max_depth': 9,\n        'n_estimators': 3000,\n        'learning_rate': 0.01,\n        'subsample': 0.9,\n        'colsample_bytree': 0.7\n    }\n}\n\n# Train age and gender-specific models\npredictions, models_dict, total_time = train_age_gender_specific_models(\n    X, y, X_test, test_ids,\n    group_params=group_params\n)\n\n# Save predictions\npredictions.to_csv('age_gender_specific_predictions.csv', index=False)\n\n# Print detailed model performance\nprint(\"\\nDetailed Model Performance:\")\nfor group_name, model_data in models_dict.items():\n    print(f\"\\n{group_name}:\")\n    print(f\"  Mean RMSLE: {model_data['mean_score']:.5f}\")\n    print(f\"  Training time: {model_data['time']/60:.1f} minutes\")\n    print(\"  Parameters:\", model_data['params'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:17:31.760494Z","iopub.execute_input":"2025-05-18T19:17:31.760709Z","iopub.status.idle":"2025-05-18T19:30:06.703246Z","shell.execute_reply.started":"2025-05-18T19:17:31.760693Z","shell.execute_reply":"2025-05-18T19:30:06.702384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filename = 'submission.csv'\npredictions.to_csv(filename, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T19:31:49.401546Z","iopub.execute_input":"2025-05-18T19:31:49.402178Z","iopub.status.idle":"2025-05-18T19:31:49.881603Z","shell.execute_reply.started":"2025-05-18T19:31:49.402156Z","shell.execute_reply":"2025-05-18T19:31:49.880842Z"}},"outputs":[],"execution_count":null}]}